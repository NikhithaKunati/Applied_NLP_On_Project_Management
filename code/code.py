# -*- coding: utf-8 -*-
"""INFO5731_Group_1_Final_Project_Harika.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/RadhavaramHarika/Harika_INFO5731_Spring2020/blob/master/INFO5731_Group_1_Final_Project_Harika.ipynb
"""

#!pip install textblob
!pip install pyspellchecker
import os, string, csv, io ,re
from google.colab import drive,files
import spacy
from gensim.corpora import Dictionary
from spacy.lang.en import English
from spacy.lang.en.stop_words import STOP_WORDS
import spacy
from spellchecker import SpellChecker
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer
from nltk.stem import WordNetLemmatizer 
from textblob import TextBlob,Word
import pandas as panda
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as nump

nlp = spacy.load("en_core_web_sm")

#Using functions for collecting extracted texts
def textContent():
    drive.mount('/content/gdrive')
    file_list = os.listdir('/content/gdrive/My Drive/Colab Notebooks/Extracted_texts')
    #filenames = os.listdir('https://github.com/NikhithaKunati/Applies_NLP_on_Project_Management/tree/master/*.docx')
    print(file_list)
    os.chdir('/content/gdrive/My Drive/Colab Notebooks/Extracted_texts')
        
    projectBackground = []
    successFactors = []
    shortcomings = []
    lessonslearned = []
    for i in range(len(file_list)):
      csvFile = open(file_list[i],'r')
      content = csv.reader(csvFile)
      if file_list[i] == 'ProjectFileData.csv': 
        next(content)   
        for each in content:
          projectBackground.append(each)
      
      elif file_list[i] == 'factors.csv':
        for row in content:
          successFactors.append(' '.join(row))
      elif file_list[i] == 'shortcomings.csv':
        for row in content:
          shortcomings.append(' '.join(row))
      else:
        for row in content:
          lessonslearned.append(' '.join(row))
      
      csvFile.close()
          
    return (projectBackground,successFactors,shortcomings,lessonslearned)

#Using functions to cleaning and preprocessing the dataset

#Function to remove all punctuations in each project report's text data
def removePunctuations(textList):
    withNoPunct = []
    transltr_table = str.maketrans('','',string.punctuation)
    for each in textList:
        each_noPunct= re.sub(r"\W"," ",each.lower().strip().translate(transltr_table))
        withNoPunct.append(each_noPunct)
    return withNoPunct

#Function to remove all numbers in the text
def removingNumbers(textList):
  noNum = []
  for each in textList:
    each = re.sub(r"\d+","",each)
    noNum.append(each)
  return noNum

#Function to split each project report's text data into tokens/words
def textTokens(textList):
    tokens = []
    for each in textList:
        sp_tokens = nlp(each)
        tblob = TextBlob(each)
        tokens.append([each for each in sp_tokens])
    return tokens

#Function to remove all stopwords in 'English' language in each project report
def removeStopWords(textTokenList):
  stop_words = stopwords.words("english")
  withnostops = []
  for each_list in textTokenList:
    withnostops.append([tokens for tokens in each_list
                    if tokens.text not in stop_words])

  return withnostops

#Function to remove most frequently occurred 5 words and most rarely occurred 5 words in each project report
def freq_rare_words(textTokenList):
  withnofreqRare = []
  for each_list in textTokenList:
    temp = []
    freq_dist = nltk.FreqDist([each.text for each in each_list])
    freqwords = [each[0] for each in freq_dist.most_common(5)]
    rarewords = [each[0] for each in freq_dist.most_common()[-5:]]
    for token in each_list:
      if token.text not in freqwords:
        if token.text not in rarewords:
          temp.append(token)
    withnofreqRare.append(temp)

  return withnofreqRare

#Function to remove all spelling mistakes in 'English' language from each project report's text data
def spellChecking(textTokenList):
  correctedTokens = []
  for each_list in textTokenList:
    spell = SpellChecker()
    correctedTokens.append([spell.correction(token.text) for token in each_list])
  return correctedTokens

#Function to convert each word/token into its root word from all project reports
def stemming(textTokenList):
  with_stemming = []
  lancaster = LancasterStemmer()
  for each_list in textTokenList:
    with_stemming.append([lancaster.stem(tokens) for tokens in each_list])
  return with_stemming

#Function to convert each word/token into its lemmatized word based on the POS tag from all project reports
def lemmatized(textTokenList):
  with_lemmas = []
  for each_list in textTokenList:
    lemmatizer = WordNetLemmatizer()
    with_lemmas.append(' '.join([token.lemma_ for token in each_list]))
   # with_lemmas.append(' '.join([lemmatizer.lemmatize(token) for token in each_list]))
  return with_lemmas

#Function to upload all the preprocessed data in all project reports into csv file using dataframe 
def uploadToCSV(projDocData):
  noPunct = removePunctuations(projDocData)
  removedNumbers = removingNumbers(noPunct)
  tokens = textTokens(removedNumbers)
  no_stopwords = removeStopWords(tokens)
  no_freqrare = freq_rare_words(no_stopwords)
  correctedWords = spellChecking(no_freqrare)
  stemmed = stemming(correctedWords)
  lemmas = lemmatized(no_freqrare)

  datafrm = panda.DataFrame({"Project data": projDocData,
                            "Removed Punctuations":noPunct,
                            "No Numbers":removedNumbers,
                            "Tokens":tokens,
                            "No stopwords":no_stopwords,
                            "No Frequent or Rare words":no_freqrare,
                            "With spell check":correctedWords,
                            "Stemmitized Words": stemmed,
                            "Lemmatized":lemmas})

  datafrm.to_csv("/content/gdrive/My Drive/Colab Notebooks/Cleaned_factors.csv")
  return datafrm

#Calling above function to collect the required data from all project reports  
data = textContent()

print('Project Background: ',data[0])
print('Factors contributing to project success: ',data[1])
print('Shortcomings in the projects: ',data[2])
print('Lessons learned from shortcomings in the project: ',data[3])

#Calling above function to upload the preprocessed data from project reports  
datafr = uploadToCSV(data[1])

datafr

import heapq

#Using functions to calculate the document-term weights for all the words in all project reports

#Function to calculate the word/counts for every word in each project report
def wordFreq(data):
  wordfreq = {}
  for document in data['Lemmatized']:
      tokens = nltk.word_tokenize(document)
      for token in tokens:
          if token not in wordfreq.keys():
              wordfreq[token] = 1
          else:
              wordfreq[token] += 1

  return wordfreq

#Function to calculate the inverse-document frequencies for every project report for all words
def idf_values(most_freq, corpus):
  word_idf_values = {}
  for token in most_freq:
    doc_containing_word = 0
    for document in corpus:
        if token in nltk.word_tokenize(document):
            doc_containing_word += 1
    word_idf_values[token] = nump.log(len(corpus)/(1 + doc_containing_word)) 
  return word_idf_values

#Function to calculate the term frequencies for every word in all project reports
def tf_values(most_freq,corpus):
  word_tf_values = {}
  for token in most_freq:
    document_tf_vector = []
    for document in corpus:
      doc_freq = 0
      for word in nltk.word_tokenize(document):
        if token == word:
          doc_freq += 1
      word_tf = doc_freq/len(nltk.word_tokenize(document))
      document_tf_vector.append(word_tf)
    word_tf_values[token] = document_tf_vector
  return word_tf_values

#Function to calculate the document_term weights for every word in all project reports
def tf_idf_values(word_tf,word_idf):
  tfidf_values = {}
  for token in word_tf.keys():
    tfidf_documents = []
    for tf_sentence in word_tf[token]:
      tf_idf_score = tf_sentence * word_idf[token]
      tfidf_documents.append(tf_idf_score)

    tfidf_values[token]= tfidf_documents
  return tfidf_values

#Calling above wordFreq function to get word counts and calculating most frequently occurred 200 words
most_freq = heapq.nlargest(200, wordFreq(datafr), key=wordFreq(datafr).get)

#Calling above tf_idf_values function to get document-term weights for most frequently occurred 200 words
result = tf_idf_values(tf_values(most_freq,datafr['Lemmatized']),idf_values(most_freq,datafr['Lemmatized']))
print(result)

#Restoring the resultant document-term frquencies into a csv file using pandas
document_datafr = panda.DataFrame(result)
document_datafr.to_csv('/content/gdrive/My Drive/Colab Notebooks/Document_term_weights_for_factors.csv')
document_datafr.max()

# import the new one
from PIL import Image
from wordcloud import WordCloud 
import matplotlib.pyplot as plt 


#Function to generate the wordcloud to determine most common words in the project reports
def generateWordCloud(data):
  drive.mount('/content/gdrive')
  file_list = os.listdir('/content/gdrive/My Drive/Colab Notebooks')
  #filenames = os.listdir('https://github.com/NikhithaKunati/Applies_NLP_on_Project_Management/tree/master/*.docx')
  print(file_list)
  os.chdir('/content/gdrive/My Drive/Colab Notebooks')
  bulb_mask = nump.array(Image.open("/content/gdrive/My Drive/Colab Notebooks/bulb_mask.png"))

  text = ' '.join(data['Lemmatized'])
  
  wc = WordCloud(background_color="white", max_words = 200, width=1600, height=800).generate(text)
  return wc


#Ploting and prnting results of above generated wordcloud with specific dimensions
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(generateWordCloud(datafr), interpolation= 'bilinear')
plt.axis("off") 
plt.tight_layout(pad = 0) 
plt.savefig("factors_m.png")

plt.show()